{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9d50c9",
   "metadata": {},
   "source": [
    "## Building a search engine\n",
    "\n",
    "Today we will write code to search a small corpus of documents for anything you are interested in. The corpus are the roughly 6,000 ``Featured Articles`` on Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126ed45",
   "metadata": {},
   "source": [
    "### Exercise 1: Text Preprocessing\n",
    "To warm up, finish three of the four functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48fca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "stopwords = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
    "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\",\n",
    "    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
    "    \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "])\n",
    "\n",
    "\n",
    "def to_lowercase(text):\n",
    "    '''\n",
    "    Takes a string, and returns it with every character lowercase\n",
    "    For example, it should map AABBccDD to aabbccdd\n",
    "    '''\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    '''\n",
    "    Takes a string and returns that string with all punctuation removed.\n",
    "    '''\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    Takes a string and returns a list of words or 'tokens' in that string, in order\n",
    "    Tokens are characters delimited by spaces, for example:\n",
    "    \"The quick brown fox\" -> ['The','quick','brown','fox']\n",
    "    '''\n",
    "    return list(text.split(' '))\n",
    "\n",
    "def remove_stop_words(tokenized_text):\n",
    "    '''\n",
    "    Removes common words from a list of tokens:\n",
    "    ['the','quick','brown','fox'] -> ['quick','brown','fox']\n",
    "    '''\n",
    "    return list(filter(lambda x: x not in stopwords, tokenized_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d34b6",
   "metadata": {},
   "source": [
    "### Exercise 2: Document Representation\n",
    "Now we need to write a function that takes a list of documents, called a `Corpus`, and generates a sorted list of its vocabulary. In other words, we need a list of all the words that appear in all of our documents.\n",
    "\n",
    "Then we need to define a function takes a document and a corpus vocabulary, and generates a word vector for that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aeb2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(documents):\n",
    "    '''\n",
    "    Takes a list of documents and returns a sorted list of \n",
    "    all the words in any of those documents.\n",
    "    '''\n",
    "    vocabulary = set()\n",
    "    for doc in documents:\n",
    "        words = remove_stop_words(tokenize(remove_punctuation(to_lowercase(doc))))\n",
    "        vocabulary.update(words) #add them to the vocabulary\n",
    "    return sorted(vocabulary) #return the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2450c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def document_to_vector(document, vocabulary):\n",
    "    '''\n",
    "    Takes a specific document and the sorted corpus-wide \n",
    "    vocabulary, and returns a vector\n",
    "    counting how many times each words appears in the specific document.\n",
    "    '''\n",
    "    word_count = {}\n",
    "    for word in vocabulary:\n",
    "        word_count[word] = 0\n",
    "    \n",
    "    words = remove_stop_words(tokenize(remove_punctuation(to_lowercase(document))))\n",
    "    for word in words: \n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "    return np.array([word_count[word] for word in vocabulary])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faefc5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [\n",
    "    'there once was a cat in hungary, who wore a hat',\n",
    "    'a quick brown fox',\n",
    "    'jumped over the lazy dog',\n",
    "    'how now brown cow',\n",
    "    'once upon a time, there was a big big big big bad wolf',\n",
    "    'It was raining cats and dogs, before it was sunny'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d7f7dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'big', 'brown', 'cat', 'cats', 'cow', 'dog', 'dogs', 'fox', 'hat', 'hungary', 'jumped', 'lazy', 'quick', 'raining', 'sunny', 'time', 'upon', 'wolf', 'wore']\n"
     ]
    }
   ],
   "source": [
    "print(create_vocabulary(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "058392a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = create_vocabulary(test_corpus)\n",
    "document_to_vector(test_corpus[4],vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca87dab",
   "metadata": {},
   "source": [
    "### Exercise 3: Building the Document-Term Matrix\n",
    "Now we want to construct a matrix where each row represents a document and each column represents a term from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3caedaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_document_term_matrix(documents, vocabulary):\n",
    "    '''\n",
    "    This function loops over all the documents in the corpus, \n",
    "    extracts the vocabulary vector of each one, and adds it to\n",
    "    list. Finally, it transforms this list of vectors into a matrix.\n",
    "    '''\n",
    "    document_vectors = []\n",
    "    for doc in documents:\n",
    "        doc_vector = document_to_vector(doc, vocabulary)\n",
    "        document_vectors.append(doc_vector)\n",
    "    return np.array(document_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0b4a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there once was a cat in hungary, who wore a hat', 'a quick brown fox', 'jumped over the lazy dog', 'how now brown cow', 'once upon a time, there was a big big big big bad wolf', 'It was raining cats and dogs, before it was sunny']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test:\n",
    "print(test_corpus)\n",
    "vocab = create_vocabulary(test_corpus)\n",
    "dtm=create_document_term_matrix(test_corpus,vocab)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd21fc",
   "metadata": {},
   "source": [
    "## Exercise 4: Search Query Processing\n",
    "Now that we have code to transform a corpus of text in a matrix, we want to be able to search the corpus via the matrix. To do so, we also need to write code that transforms a user search query into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c575871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    '''\n",
    "    A function to take a query (a string) and process it in the same way we processed\n",
    "    the texts in our corpus, getting tokens out in the end\n",
    "    '''\n",
    "    query_lower = to_lowercase(query)\n",
    "    query_no_punct = remove_punctuation(query_lower)\n",
    "    query_tokens = remove_stop_words(query_no_punct)\n",
    "    return remove_stop_words(query_tokens)\n",
    "\n",
    "def query_to_vector(query, vocabulary):\n",
    "    '''\n",
    "    A function mapping a query to a vector\n",
    "    '''\n",
    "    query_vector = np.zeros(len(vocabulary))\n",
    "    tokens = preprocess_query(query)\n",
    "    for token in tokens: \n",
    "        if token in vocabulary:\n",
    "            index = vocabulary.index(token)\n",
    "            query_vector[index] += 1\n",
    "    return query_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68f9e6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ed5af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'big', 'brown', 'cat', 'cats', 'cow', 'dog', 'dogs', 'fox', 'hat', 'hungary', 'jumped', 'lazy', 'quick', 'raining', 'sunny', 'time', 'upon', 'wolf', 'wore']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "909504c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_to_vector('How many cats are there in Hungary?',vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef08ba",
   "metadata": {},
   "source": [
    "### Exercise 5: Ranking Documents\n",
    "Now we have vectors for documents, and vectors for potential queries about those documents. Given a query we want to know: what is the closest document? Technically: what document vector is closest to the query vector?\n",
    "\n",
    "One popular measure of the similarity between two vectors is cosine similarity, which uses the idea that the cosine of a small angle is small. This works even in higher dimensions!\n",
    "\n",
    "![title](cos.png)\n",
    "\n",
    "Source: https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b00e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0: #if either vector is 0, just return 0\n",
    "        return 0\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e817e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(query, document_term_matrix, vocabulary, documents):\n",
    "    '''\n",
    "    Given a query, a document-term matrix, a vocabulary, and a set of documents,\n",
    "    return the documents listed in order of relevance to the query.\n",
    "    '''\n",
    "    #note: feel free to take multiple lines to solve any part of this.\n",
    "    query_vector = query_to_vector(query, vocabulary)\n",
    "    similarities = [cosine_similarity(doc_vector, query_vector) for doc_vector in document_term_matrix]\n",
    "    ranked_documents = sorted(zip(similarities, documents), key=lambda x: x[0], reverse=True)\n",
    "    return ranked_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fd36c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there once was a cat in hungary, who wore a hat',\n",
       " 'a quick brown fox',\n",
       " 'jumped over the lazy dog',\n",
       " 'how now brown cow',\n",
       " 'once upon a time, there was a big big big big bad wolf',\n",
       " 'It was raining cats and dogs, before it was sunny']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d39d542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'there once was a cat in hungary, who wore a hat'),\n",
       " (0, 'a quick brown fox'),\n",
       " (0, 'jumped over the lazy dog'),\n",
       " (0, 'how now brown cow'),\n",
       " (0, 'once upon a time, there was a big big big big bad wolf'),\n",
       " (0, 'It was raining cats and dogs, before it was sunny')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_documents('who is the biggest brown lazy cat in hungary?',dtm,vocab,test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080ec86",
   "metadata": {},
   "source": [
    "## Putting it all together.\n",
    "What's missing? Well... real data! I've given you a folder containing the text of 6000+ featured Wikipedia articles. Let's try to make a document term matrix out of this and query it.\n",
    "\n",
    "Just run this code, no need for coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb685202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read in!\n",
      "vocab vector generated!\n",
      "dtm generated!\n"
     ]
    }
   ],
   "source": [
    "import glob #glob is a library that helps read in lots of files\n",
    "\n",
    "new_texts=[]\n",
    "for text_file in glob.glob('data/*.txt')[0:1000]: #glob creates a list of files following a pattern\n",
    "    #note you could make this run faster by considering only the first 1000 articles in the line above.\n",
    "    with open(text_file,'r',encoding='utf-8') as f:\n",
    "        new_texts.append((''.join(f.readlines()))[0:3000])  #cap the file at 3000 lines\n",
    "print('data read in!')\n",
    "\n",
    "vocab2=create_vocabulary(new_texts)\n",
    "print('vocab vector generated!')\n",
    "        \n",
    "dtm2=create_document_term_matrix(new_texts,vocab2)\n",
    "print('dtm generated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46bf3fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 38978)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8613cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06899341558261021,\n",
       " 'Barbara L (1947–1977) was an American Quarter Horse that raced during the early 1950s and often defeated some of the best racehorses of the time. She earned $32,836 (equivalent to $370,000 in 2023) on the race track in 81 starts and 21 wins, including six wins in stakes races. She set two track records during her racing career. After retiring from racing in 1955, she went on to become a broodmare and had 14 foals, including 11 who earned their Race Register of Merit with the American Quarter Horse Association (AQHA). Her offspring earned more than $200,000 in race money. She died in 1977 and was inducted into the AQHA\\'s American Quarter Horse Hall of Fame in 2007.\\n\\n\\n== Early life ==\\nBarbara L was foaled in 1947, a bay daughter of a Thoroughbred stallion named Patriotic and a Quarter Horse broodmare named Big Bess. She was registered with the AQHA as number 146,954. Her sire, or father, was a grandson of Man o\\' War, while her dam, or mother, descended from the Quarter Horse Peter McCue. Barbara L was registered as bred by James Hunt of Sonora, Texas, and her owner at the time of registration was A. B. Green, of Purcell, Oklahoma.\\nAs a yearling, Barbara L was sold at auction for $140 (equivalent to $1,800 in 2023) to a Mr. Lumpkin, who sold horse trailers for a living. She spent the next period of her life demonstrating trailers across Texas before someone suggested that Lumpkin race her. Lumpkin changed her name from \"Anthem\" to \"Barbara L\" in honor of his daughter Barbara, who was the filly\\'s first trainer.\\n\\n\\n== Racing career ==\\nBarbara L\\'s first race was in 1949 at Del Rio, Texas, where she came in fourth and only rated a B speed index (a measure of how fast a horse ran in a race). She did not win a race until her third start that year, completing a 440 yards (400 m) course in 23.4 seconds. In the following years, she raced at Centennial Race Track in Colorado; at Raton in New Mexico; at Albuquerque, New Mexico; at Phoenix, Arizona; at Los Alamitos Race Track in California; and at Bay Meadows Race Track. She raced for Lumpkin until 1952, when she was sold to A. B. Green. Green raced her until May 1955, when her last start was recorded with the AQHA.\\nBarbara L raced for seven years, starting 81 times. She ended her career on the track with 21 victories, 23 seconds and nine third-place finishes. During her racing career, she beat a number of the top racehorses of her time: Stella Moore, Blob Jr, Bart BS, Johnny Dial, and Monita. She won six stakes races, placed second in four, and came in third in three. Her earnings on the racetrack were $32,836 (equivalent to $370,000 in 2023). The stakes wins were the Speedwell Handicap, the Del Rio Feature, the Bart BS Stakes, the Miss Princess Invitational Handicap, Maddon\\'s Bright Eyes Handicap, and the Pima County Fair Premier Stakes. She set two track records—one at Centennial for 400 yards (370 m) with 20.2 seconds, the other at Los Alamitos for 400 yards (370 m) with 20.5 seconds—and equaled the 350-yard')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_documents('What is the meaning of life?',dtm2,vocab2,new_texts)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4006f752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1118033988749895,\n",
       " 'The banded sugar ant (Camponotus consobrinus), also known as the sugar ant, is a species of ant native to Australia. A member of the genus Camponotus in the subfamily Formicinae, it was described by German entomologist Wilhelm Ferdinand Erichson in 1842. Its common name refers to the ant\\'s liking for sugar and sweet food, as well as the distinctive orange-brown band that wraps around its gaster.\\nThe ant is polymorphic and relatively large, with two different castes of workers: major workers (also known as soldiers), and minor workers. These two group of workers measure around 5 to 15 millimetres (0.2 to 0.6 in) in length, while the queen ants are even larger. Mainly nocturnal, banded sugar ants prefer a mesic habitat, and are commonly found in forests and woodlands. They also occur in urban areas, where they are considered a household pest. The ant\\'s diet includes sweet secretions that are retrieved from aphids and other insects that it tends. This species is a competitor of the meat ant (Iridomyrmex purpureus), and food robbery and nest-plugging is known to occur between these two ants. Workers prey on insects, killing them with a spray of formic acid. Banded sugar ants are preyed upon by other ants, echidnas, and birds. The eggs of this species were consumed by Indigenous Australians.\\n\\n\\n== Taxonomy ==\\nThe banded sugar ant was first described by German entomologist Wilhelm Ferdinand Erichson, who named it Formica consobrina in 1842. The holotype specimen is a queen collected from Tasmania, which is now housed in the Museum für Naturkunde in Berlin. Formica consobrina was later moved to the genus Camponotus as Camponotus consobrinus, by entomologist Julius Roger in 1863. In 1933, American entomologist William Morton Wheeler described some subspecies and variants of the banded sugar ant. These subspecies were C. consobrinus lividipes and C. consobrinus nigriceps, while the variants were C. consobrinus var. obniger and C. consobrinus var. perthianus. Some of these classifications were short-lived; C. consobrinus nigriceps was later revived as a full species in 1934 as C. nigriceps, while C. consobrinus lividipes was synonymised with C. consobrinus. C. consobrinus lividipes was treated as a subspecies for C. nigriceps in 1985, now known as C. nigriceps lividipes. In 1996 C. consobrinus perthianus was synonymised with C. nigriceps, and C. consobrinus var. obniger was synonymised with C. consobrinus.\\nThe specific name is derived from the Latin word consobrina, meaning \"cousin\". This is in reference to its similar appearance with the species C. herculeanus.\\nThe ant is a member of the Camponotus nigriceps species group, which also includes C. clarior, C. dryandrae, C. eastwoodi, C. loweryi, C. longideclivis, C. nigriceps, C. pallidiceps and C. prostans. The species is commonly known as the banded sugar ant or sugar ant due to its attraction to sweet food and the orange-brown band that is present on its gaster.\\n\\n\\n== Description ==\\n\\nBanded sugar ants appe')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_documents('Is data science going to be a good profession in the future?',dtm2,vocab2,new_texts)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22bce6",
   "metadata": {},
   "source": [
    "### Why is this so slow?\n",
    "There are many reasons why this solution is so slow. Two reasons:\n",
    "1. This is a lot of data - check how many unique tokens there are in our corpus.\n",
    "2. The matrix is very big. How many entries do we have? (# of unique tokens x # of texts - we are counting 0s!)\n",
    "\n",
    "In a future text mining class you'll learn how to solve these kinds of issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99e68d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20000, 0.16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique words, size of the matrix, size of the data in the matrix in mb\n",
    "len(vocab), len(vocab)*len(new_texts),(len(vocab)*len(new_texts)*8)/1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61b57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
